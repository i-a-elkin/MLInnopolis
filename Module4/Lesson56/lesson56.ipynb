{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bjl799e4cQ6h"
   },
   "source": [
    "# 02. Домашнее задание. Обучение Vision Transformer\n",
    "\n",
    "\n",
    "В этом задании вам предлагается дописать реализацию ViT и обучить его на датасете CIFAR-10. Хотя эта модель гораздо больше тех CNN, что мы изучали (и будем изучать), ресурсов colab-а гарантированно хватит, чтобы закончить обучение. Так же рекомендуется просмотреть Lab_5, Lab_3 и Lab_6 , поскольку это задание во многом основано на них.\n",
    "\n",
    "**Ищите комментарии \"Write your code here\" для быстрого обнаружения мест, где вы должны что-то дописать!**\n",
    "\n",
    "![](https://hashtelegraph.com/wp-content/uploads/2024/08/shooting-sparrows-with-a-cannon--1024x576.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NzC89Cvilx5h"
   },
   "source": [
    "## 1. Patch Embeddings\n",
    "\n",
    "Отличительной особенностью ViT является эмбеддинги изображений. Чтобы представить изображение в виде входного вектора для трансформера, оно разбивается на фрагменты (patches) заданного размера. В нашей имплементации это будет сделано с помощью обучаемой свертки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1874,
     "status": "ok",
     "timestamp": 1746614188815,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "m7zN5jQweBwu"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class PatchEmbeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Преобразуйте изображение в патчи, а затем спроецируйте их в векторное пространство.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.patch_size = config[\"patch_size\"]\n",
    "        self.num_channels = config[\"num_channels\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # Количество фрагментов, исходя из размера изображения и размера фрагмента.\n",
    "        self.num_patches = (self.image_size // self.patch_size) ** 2\n",
    "        # Write your code here +\n",
    "        # Допишите нужные параметры. Выходом свертки явлются self.num_patches фрагментов размером self.hidden_size\n",
    "        # Вы можете менять параметры out_channels, kernel_size, stride\n",
    "        self.projection = nn.Conv2d(\n",
    "            self.num_channels,\n",
    "            self.hidden_size,\n",
    "            kernel_size=self.patch_size,\n",
    "            stride=self.patch_size,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (batch_size, num_channels, image_size, image_size) -> (batch_size, num_patches, hidden_size)\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.projection(x)\n",
    "        x = x.flatten(2).transpose(1, 2)\n",
    "        assert x.shape[0] == batch_size\n",
    "        assert x.shape[1] == self.num_patches\n",
    "        assert x.shape[2] == self.hidden_size\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1746614188821,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "lWKP9EtkmVff"
   },
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    Combine the patch embeddings with the class token and position embeddings.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_embeddings = PatchEmbeddings(config)\n",
    "        # Создаем обучаемый [CLS] токен\n",
    "        # Подобно BERT, токен [CLS] добавляется в начало входной последовательности.\n",
    "        # Токен используется для классификации\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, config[\"hidden_size\"]))\n",
    "        self.position_embeddings = nn.Parameter(\n",
    "            torch.randn(1, self.patch_embeddings.num_patches + 1, config[\"hidden_size\"])\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patch_embeddings(x)\n",
    "        batch_size, _, _ = x.size()\n",
    "        # Расширяем токен [CLS] до размера батча\n",
    "        # (1, 1, hidden_size) -> (batch_size, 1, hidden_size)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n",
    "        # Присоединяем токен [CLS] к началу входной последовательности.\n",
    "        # Длина последовательности (num_patches + 1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = x + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSV0lUN3mWAH"
   },
   "source": [
    "## 2. Attention head\n",
    "\n",
    "Допишите реализацию головы трансформера. Формулы подсчеты attention score можно найти в Лекции 5, посвященной трансформерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1746614188824,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "Xi2pjXPprRkB"
   },
   "outputs": [],
   "source": [
    "from torch.nn.functional import softmax\n",
    "\n",
    "\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Голова трансформера\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hidden_size, attention_head_size, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attention_head_size = attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(hidden_size, attention_head_size, bias=True)\n",
    "        self.key = nn.Linear(hidden_size, attention_head_size, bias=True)\n",
    "        self.value = nn.Linear(hidden_size, attention_head_size, bias=True)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x перемножается на query, key, value матрицы.\n",
    "        # Результат перемножения используется для для подсчета attention output\n",
    "        Q = self.query(x)\n",
    "        K = self.key(x)\n",
    "        V = self.value(x)\n",
    "        # Write your code here +\n",
    "        # Посчитайте attention_output по формуле: softmax(Q*K.T/sqrt(head_size)) * V\n",
    "        # Опционально, на softmax(Q*K.T/sqrt(head_size)) можно добавить dropout\n",
    "        attention_output = softmax(\n",
    "            torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.attention_head_size),\n",
    "            dim=-1,\n",
    "        )\n",
    "        attention_output = self.dropout(attention_output)\n",
    "        attention_output = torch.matmul(attention_output, V)\n",
    "\n",
    "        return attention_output\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention module\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        self.num_attention_heads = config[\"num_attention_heads\"]\n",
    "        # Размерность для одной головы\n",
    "        self.attention_head_size = self.hidden_size // self.num_attention_heads\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "        # Write your code here +\n",
    "        # Создайте несколько голов и добавьте их в self.heads\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(\n",
    "                    self.hidden_size,\n",
    "                    self.attention_head_size,\n",
    "                    config[\"attention_probs_dropout_prob\"],\n",
    "                )\n",
    "                for _ in range(self.num_attention_heads)\n",
    "            ]\n",
    "        )\n",
    "        # Финальная линейная проекция\n",
    "        self.output_projection = nn.Linear(self.all_head_size, self.hidden_size)\n",
    "        self.output_dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Считаем параллельно в каждой голове, потом конкатенируем результат\n",
    "        attention_outputs = [head(x) for head in self.heads]\n",
    "        attention_output = torch.cat(\n",
    "            [attention_output for attention_output in attention_outputs], dim=-1\n",
    "        )\n",
    "        attention_output = self.output_projection(attention_output)\n",
    "        attention_output = self.output_dropout(attention_output)\n",
    "        return attention_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1746614188824,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "g-L-8B01zt3S"
   },
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    \"\"\"\n",
    "    Многослойный персептрон\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dense_1 = nn.Linear(config[\"hidden_size\"], config[\"intermediate_size\"])\n",
    "        self.activation = nn.GELU()\n",
    "        self.dense_2 = nn.Linear(config[\"intermediate_size\"], config[\"hidden_size\"])\n",
    "        self.dropout = nn.Dropout(config[\"hidden_dropout_prob\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense_1(x)\n",
    "        x = self.activation(x)\n",
    "        x = self.dense_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9gBEKwqvzrEJ"
   },
   "source": [
    "## 3. Encoder\n",
    "\n",
    "Схема энкодера трансформера, используемого в ViT\n",
    "\n",
    "![](https://theaisummer.com/static/aa65d942973255da238052d8cdfa4fcd/7d4ec/the-transformer-block-vit.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1746614188826,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "JJRMILRCzqeP"
   },
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\"\n",
    "    Блок энкодера, как на рисунке сверху\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiHeadAttention(config)\n",
    "        self.layernorm_1 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "        self.mlp = MLP(config)\n",
    "        self.layernorm_2 = nn.LayerNorm(config[\"hidden_size\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Write your code here +\n",
    "        # x должен пройти по последовательности LayerNorm, MultiHeadAttention и\n",
    "        # MLP слоев, как на рисунке сверху\n",
    "        norm_x = self.layernorm_1(x)\n",
    "        attention_out = self.attention(norm_x)\n",
    "        x = x + attention_out\n",
    "\n",
    "        norm_x = self.layernorm_2(x)\n",
    "        mlp_out = self.mlp(norm_x)\n",
    "        x = x + mlp_out\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Энкодер, состоящий из config[\"num_blocks\"] блоков\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [Block(config) for _ in range(config[\"num_blocks\"])]\n",
    "        )\n",
    "        # Write your code here +\n",
    "        # Создайте список из config[\"num_blocks\"] Block-ов\n",
    "        ...\n",
    "\n",
    "    def forward(self, x):\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1746614188827,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "589LyN0K4OUF"
   },
   "outputs": [],
   "source": [
    "class ViTForClassfication(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision transformer. Состоит из PatchEmbedder-а, энкодера и линейного слоя\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # 32 для CIFAR\n",
    "        self.image_size = config[\"image_size\"]\n",
    "        self.hidden_size = config[\"hidden_size\"]\n",
    "        # 10 для CIFAR\n",
    "        self.num_classes = config[\"num_classes\"]\n",
    "\n",
    "        self.embedding = Embeddings(config)\n",
    "        self.encoder = Encoder(config)\n",
    "        self.classifier = nn.Linear(self.hidden_size, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedding_output = self.embedding(x)\n",
    "        encoder_output = self.encoder(embedding_output)\n",
    "        # Рассчываем логиты как выходные данные токена [CLS]\n",
    "        logits = self.classifier(encoder_output[:, 0, :])\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLeQnVc9-uKN"
   },
   "source": [
    "## 4. Подготовка датасета\n",
    "\n",
    "Эта часть мало чем отличается от Lab_3, поэтому можете свериться с ней"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1928,
     "status": "ok",
     "timestamp": 1746614190756,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "UbEp406Wqksp"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "def prepare_data(\n",
    "    batch_size=4, num_workers=2, train_sample_size=None, test_sample_size=None\n",
    "):\n",
    "    # Write your code here +\n",
    "    # Допишите нужные трансформации для обучающего датасета\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomRotation(10),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=train_transform\n",
    "    )\n",
    "    if train_sample_size is not None:\n",
    "        # Randomly sample a subset of the training set\n",
    "        indices = torch.randperm(len(trainset))[:train_sample_size]\n",
    "        trainset = torch.utils.data.Subset(trainset, indices)\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    test_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=test_transform\n",
    "    )\n",
    "    if test_sample_size is not None:\n",
    "        indices = torch.randperm(len(testset))[:test_sample_size]\n",
    "        testset = torch.utils.data.Subset(testset, indices)\n",
    "\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBu4lpCHRGEs"
   },
   "source": [
    "# Finally, обучение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 288151,
     "status": "ok",
     "timestamp": 1746614478909,
     "user": {
      "displayName": "Иван Елькин",
      "userId": "12386214062098670036"
     },
     "user_tz": -300
    },
    "id": "qGHq-Nl0rrFF",
    "outputId": "ae4461d1-9fe8-437f-c6cb-ca71185fcc52"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170M/170M [00:10<00:00, 16.1MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Train loss: 1.7524, Test loss: 1.5415, Accuracy: 0.4374\n",
      "Epoch: 2, Train loss: 1.4571, Test loss: 1.3485, Accuracy: 0.5074\n",
      "Epoch: 3, Train loss: 1.3401, Test loss: 1.2912, Accuracy: 0.5272\n",
      "Epoch: 4, Train loss: 1.2640, Test loss: 1.2533, Accuracy: 0.5455\n",
      "Epoch: 5, Train loss: 1.2157, Test loss: 1.2567, Accuracy: 0.5468\n",
      "Epoch: 6, Train loss: 1.1663, Test loss: 1.2361, Accuracy: 0.5615\n",
      "Epoch: 7, Train loss: 1.1250, Test loss: 1.0990, Accuracy: 0.6052\n",
      "Epoch: 8, Train loss: 1.0950, Test loss: 1.0420, Accuracy: 0.6245\n",
      "Epoch: 9, Train loss: 1.0591, Test loss: 1.0592, Accuracy: 0.6164\n",
      "Epoch: 10, Train loss: 1.0274, Test loss: 1.1105, Accuracy: 0.6032\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "epochs = 10\n",
    "lr = 1e-2\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Можете по-экспериментировать с параметрами для улучшения обучения\n",
    "config = {\n",
    "    \"patch_size\": 4,  # Input image size: 32x32 -> 8x8 patches\n",
    "    \"hidden_size\": 48,\n",
    "    \"num_blocks\": 4,\n",
    "    \"num_attention_heads\": 4,\n",
    "    \"intermediate_size\": 4 * 48,  # 4 * hidden_size\n",
    "    \"hidden_dropout_prob\": 0.0,\n",
    "    \"attention_probs_dropout_prob\": 0.0,\n",
    "    \"initializer_range\": 0.02,\n",
    "    \"image_size\": 32,\n",
    "    \"num_classes\": 10,  # num_classes of CIFAR10\n",
    "    \"num_channels\": 3,\n",
    "}\n",
    "\n",
    "assert config[\"hidden_size\"] % config[\"num_attention_heads\"] == 0\n",
    "assert config[\"intermediate_size\"] == 4 * config[\"hidden_size\"]\n",
    "assert config[\"image_size\"] % config[\"patch_size\"] == 0\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    \"\"\"\n",
    "    Класс обучения\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, loss_fn, device):\n",
    "        self.model = model.to(device)\n",
    "        self.optimizer = optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "        self.device = device\n",
    "\n",
    "    def train(self, trainloader, testloader, epochs):\n",
    "        for i in range(epochs):\n",
    "            self.model.train()\n",
    "            train_loss = 0\n",
    "            for batch in trainloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "                # Zero the gradients\n",
    "                self.optimizer.zero_grad()\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(self.model(images), labels)\n",
    "                # Backpropagate the loss\n",
    "                loss.backward()\n",
    "                # Update the model's parameters\n",
    "                self.optimizer.step()\n",
    "                train_loss += loss.item() * len(images)\n",
    "            accuracy, test_loss = self.evaluate(testloader)\n",
    "            print(\n",
    "                f\"Epoch: {i+1}, Train loss: {train_loss / len(trainloader.dataset):.4f}, Test loss: {test_loss:.4f}, Accuracy: {accuracy:.4f}\"\n",
    "            )\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate(self, testloader):\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in testloader:\n",
    "                # Move the batch to the device\n",
    "                batch = [t.to(self.device) for t in batch]\n",
    "                images, labels = batch\n",
    "\n",
    "                # Get predictions\n",
    "                logits = self.model(images)\n",
    "\n",
    "                # Calculate the loss\n",
    "                loss = self.loss_fn(logits, labels)\n",
    "                total_loss += loss.item() * len(images)\n",
    "\n",
    "                # Calculate the accuracy\n",
    "                predictions = torch.argmax(logits, dim=1)\n",
    "                correct += torch.sum(predictions == labels).item()\n",
    "        accuracy = correct / len(testloader.dataset)\n",
    "        avg_loss = total_loss / len(testloader.dataset)\n",
    "        return accuracy, avg_loss\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Training parameters\n",
    "    # Load the CIFAR10 dataset\n",
    "    trainloader, testloader = prepare_data(batch_size=batch_size)\n",
    "    # Create the model, optimizer, loss function and trainer\n",
    "    model = ViTForClassfication(config)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-2)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "    trainer = Trainer(model, optimizer, loss_fn, device=device)\n",
    "    trainer.train(trainloader, testloader, epochs)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1uaP8KNA4SX53E3gnTA-pseMIth04UojK",
     "timestamp": 1746550504745
    }
   ]
  },
  "kernelspec": {
   "display_name": ".mlinnopolis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
